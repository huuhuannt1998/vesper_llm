# VESPER LLM - Smart House Navigation System

![Version](https://img.shields.io/badge/version-1.0.0-blue.svg)
![Python](https://img.shields.io/badge/python-3.8+-green.svg)
![Blender](https://img.shields.io/badge/blender-4.0+-orange.svg)
![License](https://img.shields.io/badge/license-MIT-green.svg)

VESPER LLM is an intelligent house navigation system that combines Large Language Models (LLM) with Blender's 3D environment to create realistic, AI-controlled actor movement through virtual house environments.

## ðŸš€ Features

### Core Functionality
- **ðŸ¤– LLM-Controlled Navigation**: AI determines optimal room visitation order based on tasks
- **ðŸ“¸ Bird's Eye View Analysis**: Real-time screenshot capture for visual feedback
- **ðŸš¶ Realistic Human Movement**: Step-by-step movement with human-like timing and pace
- **ðŸŽ¯ Task-Based Planning**: 6 different daily routine types (Morning, Evening, Cleaning, etc.)
- **ðŸ  Smart House Integration**: Pre-configured room layouts with navigation logic
- **ðŸ“Š LLM Evaluation System**: Comprehensive correctness testing with 6 evaluation methods

### Technical Features
- **Real-time LLM Integration**: OpenAI-compatible API support
- **Blender Addon System**: Seamless P-key activation in 3D viewport
- **Visual Feedback Loop**: Screenshot â†’ LLM Analysis â†’ Movement Commands
- **Fallback System**: Works offline with rule-based navigation
- **Game Engine Integration**: Smooth transition to Blender Game Engine
- **Standalone Evaluation**: Independent LLM testing without Blender dependency

## ðŸ—ï¸ Architecture

```
vesper_llm/
â”œâ”€â”€ backend/                 # LLM integration and API handling
â”‚   â””â”€â”€ app/
â”‚       â””â”€â”€ llm/
â”‚           â”œâ”€â”€ client.py    # LLM communication client
â”‚           â””â”€â”€ planner.py   # Task planning logic
â”œâ”€â”€ blender/                 # Blender integration
â”‚   â””â”€â”€ addons/
â”‚       â””â”€â”€ vesper_tools/
â”‚           â””â”€â”€ __init__.py  # Main Blender addon
â”œâ”€â”€ evaluation/              # LLM correctness evaluation system
â”‚   â”œâ”€â”€ simple_evaluator.py # Standalone evaluation with 6 methods
â”‚   â”œâ”€â”€ metrics.py          # Research metrics and statistics
â”‚   â””â”€â”€ quick_eval.py       # Quick evaluation runner
â”œâ”€â”€ configs/                 # Configuration files
â”œâ”€â”€ scripts/                 # Utility scripts
â””â”€â”€ requirements.txt         # Python dependencies
```

## ðŸ› ï¸ Installation

### Prerequisites
- **Blender 4.0+** (UPBGE recommended)
- **Python 3.8+**
- **LLM Server** (OpenAI-compatible API)

### Setup Steps

1. **Clone the repository**
   ```bash
   git clone https://github.com/huuhuannt1998/vesper_llm.git
   cd vesper_llm
   ```

2. **Install Python dependencies**
   ```bash
   pip install -r requirements.txt
   ```

3. **Configure LLM server**
   ```bash
   cp .env.example .env
   # Edit .env with your LLM server details
   ```

4. **Install Blender addon**
   - Open Blender
   - Go to `Edit > Preferences > Add-ons`
   - Click "Install..." and select `blender/addons/vesper_tools/__init__.py`
   - Enable "VESPER Tools" addon

## ðŸŽ® Usage

### Basic Operation

1. **Load your house.blend file** in Blender
2. **Ensure you have an actor object** (named "Actor", "Human", or similar)
3. **Press P key** in the 3D Viewport
4. **Watch the AI-controlled navigation!**

### Expected Behavior

```
ðŸŽ¯ VESPER LLM NAVIGATION TRIGGERED!
ðŸ“‹ Selected 3 Random Tasks: ['Make coffee', 'Watch TV', 'Go to bed']
ðŸ§  LLM Response: ["Kitchen", "LivingRoom", "Bedroom"]
ðŸš¶ Found actor: Actor at [-2.40, 1.10]

ðŸ“¸ Bird's eye screenshot captured
ðŸš¶ Starting realistic human movement to Kitchen
  Step 1: Actor at [-2.40, 1.10], Distance: 4.42
  ðŸ“¡ Movement: RIGHT (small human step)
  ...continues with realistic movement...
  ðŸŽ¯ Reached Kitchen in 18 steps!

ðŸŽ® Starting Game Engine...
âœ… Game Engine started successfully!
```

### Room Configuration

The system supports these predefined rooms:
- **LivingRoom**: `[-2.0, 1.5]`
- **Kitchen**: `[2.0, 1.5]`
- **Bedroom**: `[-3.0, -2.0]`
- **Bathroom**: `[1.0, -2.0]`
- **DiningRoom**: `[0.0, 1.0]`
- **Office**: `[3.0, 3.0]`

## ðŸ”§ Configuration

### LLM Server Setup

Configure your LLM server in the `.env` file:

```env
LLM_API_URL=http://your-llm-server:8080/api/chat/completions
LLM_API_KEY=your-api-key
LLM_MODEL=openai/gpt-oss-20b
LLM_REQUEST_TIMEOUT=30
LLM_MAX_TOKENS=256
```

### Movement Parameters

Adjust movement realism in the addon:
- **Step size**: `0.12` units (realistic human steps)
- **Step timing**: `0.4` seconds between steps
- **Max steps**: `25` steps per room
- **Tolerance**: `0.3` units accuracy

## ðŸ“Š Task Routines

The system includes 6 predefined daily routines:

1. **Morning Routine**: Wake up â†’ Brush teeth â†’ Make coffee
2. **Evening Routine**: Turn on TV â†’ Dim lights â†’ Go to bedroom
3. **Cleaning Routine**: Check kitchen â†’ Tidy living room â†’ Make bed
4. **Work Break**: Get coffee â†’ Check TV news â†’ Return to work area
5. **Guest Preparation**: Clean living room â†’ Prepare coffee â†’ Check bedroom
6. **Relaxation Time**: Turn off lights â†’ Watch TV â†’ Go to bed

## ðŸŽ¯ API Reference

### Core Functions

#### `chat_completion(system: str, user: str) -> str`
Communicates with LLM server for task planning and navigation decisions.

#### `execute_self_contained_navigation()`
Main navigation loop with LLM integration and visual feedback.

#### `capture_birds_eye_view() -> str`
Captures top-down screenshot for visual analysis.

#### `move_actor_step_by_step(actor, target_room, target_pos)`
Executes realistic human-like movement between rooms.

## ðŸ›¡ï¸ Error Handling

The system includes comprehensive fallback mechanisms:
- **LLM Unavailable**: Falls back to rule-based room selection
- **Screenshot Failure**: Continues with direct pathfinding
- **Movement Blocked**: Skips to next room after timeout
- **Game Engine Issues**: Continues in Edit mode

## ðŸ“Š LLM Correctness Evaluation

### Overview

VESPER includes a comprehensive **standalone evaluation system** that measures LLM correctness in navigation tasks through 6 different testing methods. This evaluation framework is perfect for research papers and system validation.

### ðŸ”¬ Evaluation Methods

The evaluation system assesses LLM performance across multiple dimensions:

1. **ðŸ“ Task-to-Room Mapping Accuracy** - Tests basic navigation understanding
2. **ðŸ—ºï¸ Spatial Reasoning Assessment** - Evaluates logical spatial decision making
3. **ðŸ“‹ Multi-step Task Planning** - Measures complex sequence planning capability
4. **ðŸ§  Context Understanding** - Tests implicit reasoning from situational context
5. **âš ï¸ Error Handling and Edge Cases** - Validates robustness with invalid inputs
6. **â±ï¸ Response Consistency and Reliability** - Ensures repeatable performance

### ðŸš€ Running the Evaluation

#### Quick Evaluation (Standalone)
```bash
# Navigate to evaluation directory
cd evaluation

# Run standalone evaluation (no external dependencies)
python simple_evaluator.py
```

#### Expected Output
```
ðŸ”¬ VESPER LLM Navigation Evaluation
==================================================
ðŸ“ Method 1: Task-to-Room Mapping Accuracy
  Testing systematic task-to-room associations...
    âœ… 'make coffee' â†’ Kitchen (confidence: 0.95)
    âœ… 'watch television' â†’ LivingRoom (confidence: 0.92)
    ðŸ“Š Task-Room Mapping Accuracy: 90.0%

ðŸ—ºï¸ Method 2: Spatial Reasoning Assessment
  Testing spatial reasoning and navigation planning...
    âœ… Closest room to Kitchen for water â†’ DiningRoom
    ðŸ“Š Spatial Reasoning Accuracy: 100.0%

ðŸ“‹ Method 3: Multi-step Task Planning
  Testing multi-step task planning and sequencing...
    Task: Morning routine
      Predicted: ['Bedroom', 'Bathroom', 'Kitchen', 'Office']
      Expected:  ['Bedroom', 'Bathroom', 'Kitchen', 'Office']
      Similarity: 1.00
    ðŸ“Š Multi-step Planning Accuracy: 84.4%

ðŸ“Š LLM CORRECTNESS EVALUATION RESULTS
============================================================
ðŸŽ¯ Overall LLM Correctness Score: 85.0%
ðŸ“ Task Mapping Accuracy: 90.0%
ðŸ—ºï¸ Spatial Reasoning: 100.0%
ðŸ“‹ Multi-step Planning: 84.4%
ðŸ§  Context Understanding: 80.0%
âš ï¸ Error Handling: 83.3%
â±ï¸ Response Consistency: 93.3%

ðŸ’¡ Assessment: Good
ðŸ“ Full report saved: vesper_llm_evaluation_20250813_154305.json
```

### ðŸ“ˆ Research Integration

#### Generated Report Structure
```json
{
  "metadata": {
    "evaluation_type": "LLM Navigation Correctness Assessment",
    "evaluation_date": "2025-08-13T15:43:05.629462",
    "vesper_version": "2.3.0",
    "evaluation_methods": 6,
    "total_test_cases": 44
  },
  "llm_correctness_metrics": {
    "overall_correctness_score": 0.85,
    "task_mapping_accuracy": 0.90,
    "spatial_reasoning_accuracy": 1.00,
    "multi_step_planning_accuracy": 0.84,
    "context_understanding_accuracy": 0.80,
    "error_handling_rate": 0.83,
    "response_consistency": 0.93
  }
}
```

#### For Research Papers

The evaluation system provides:
- **âœ… Quantitative Metrics** - Statistical validation with confidence scores
- **âœ… Comprehensive Testing** - 44+ test cases across 6 different methods
- **âœ… Research-Ready Data** - JSON reports with metadata and detailed results
- **âœ… Reproducible Results** - Consistent evaluation framework
- **âœ… Baseline Comparisons** - Performance benchmarking

### ðŸŽ¯ Evaluation Test Cases

#### Task-to-Room Mapping Tests
```python
# Clear mappings
"make coffee" â†’ Kitchen (95% confidence)
"brush teeth" â†’ Bathroom (98% confidence)
"watch television" â†’ LivingRoom (92% confidence)

# Ambiguous cases
"read a book" â†’ [LivingRoom, Bedroom] (75% confidence)
"make a phone call" â†’ [Office, LivingRoom] (72% confidence)
```

#### Spatial Reasoning Tests
- Proximity analysis: "Closest room to Kitchen for water"
- Path optimization: "Most efficient route planning"
- Activity sequences: "Morning routine spatial logic"

#### Multi-step Planning Tests
- **Morning Routine**: Bedroom â†’ Bathroom â†’ Kitchen â†’ Office
- **Evening Routine**: Office â†’ DiningRoom â†’ LivingRoom â†’ Bedroom
- **Work Break**: Office â†’ Kitchen â†’ Office
- **Guest Preparation**: Kitchen â†’ DiningRoom â†’ LivingRoom

### ðŸ“‹ Evaluation Configuration

#### Custom Test Cases
You can extend the evaluation by modifying `simple_evaluator.py`:

```python
# Add custom task mappings
custom_tests = [
    {"task": "your_custom_task", "expected": "TargetRoom", "confidence": 0.85},
    # Add more test cases...
]
```

#### Performance Benchmarks
- **Excellent**: Overall score > 90%
- **Good**: Overall score 80-90%
- **Needs Improvement**: Overall score < 80%

### ðŸ”§ Advanced Evaluation

#### With Real LLM Server
```python
# Configure LLM server in evaluator
LLM_SERVER = "http://cci-siscluster1.charlotte.edu:8080/api/chat/completions"
MODEL = "openai/gpt-oss-20b"

# Run evaluation with live LLM queries
python standalone_evaluator.py  # Full server integration
```

#### Batch Testing
```python
# Run multiple evaluation rounds for statistical analysis
for i in range(10):
    evaluator.run_evaluation_suite()
```

## ðŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ðŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ðŸ™ Acknowledgments

- Built with [Blender](https://www.blender.org/) 3D creation suite
- Powered by OpenAI-compatible LLM APIs
- Inspired by smart home automation and AI-driven navigation

---

**VESPER LLM v1.0.0** - Bringing AI intelligence to virtual house navigation ðŸ ðŸ¤–
